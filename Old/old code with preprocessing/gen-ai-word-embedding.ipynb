{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T11:43:28.641473Z",
     "iopub.status.busy": "2025-03-10T11:43:28.641148Z",
     "iopub.status.idle": "2025-03-10T11:43:32.069393Z",
     "shell.execute_reply": "2025-03-10T11:43:32.068699Z",
     "shell.execute_reply.started": "2025-03-10T11:43:28.641450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.0)\n",
      "Requirement already satisfied: youtube-transcript-api in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: spacy in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.48.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.3.1+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.27.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (2024.8.30)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp world\\appdata\\roaming\\python\\python312\\site-packages (from pinecone-client) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (2.2.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp world\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp world\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->youtube-transcript-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->youtube-transcript-api) (3.10)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp world\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp world\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp world\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "!pip install sentence-transformers pinecone-client youtube-transcript-api spacy nltk\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Check for GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"pcsk_3rWW1w_Eua9C9tD1rbQybpChVD9nDijUycon7auXNs3afy7T2Z2zK2YnSHEFeLmKJsx4pp\")\n",
    "\n",
    "# Create/connect to index\n",
    "index_name = \"video-embeddings\"\n",
    "\n",
    "# Check if index exists, create if not\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # all-MiniLM-L6-v2 outputs 384-dimensional embeddings\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "    # Wait for index to be ready\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Load model on GPU if available\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Load SpaCy model for NLP tasks\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_transcript(video_id):\n",
    "    \"\"\"Extracts transcript from a YouTube video.\"\"\"\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        formatter = TextFormatter()\n",
    "        return formatter.format_transcript(transcript)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess the text data.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def pos_tagging(text):\n",
    "    \"\"\"Perform POS tagging on the given text and return analysis.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract POS counts\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    \n",
    "    # Extract key entities and their POS\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Extract key noun phrases (potential skills/topics)\n",
    "    noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    # Extract frequent verbs (actions)\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    verb_counts = Counter(verbs)\n",
    "    \n",
    "    return {\n",
    "        \"pos_distribution\": dict(pos_counts),\n",
    "        \"entities\": entities,\n",
    "        \"noun_phrases\": noun_chunks,\n",
    "        \"common_verbs\": dict(verb_counts.most_common(10))\n",
    "    }\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Generates embedding using Sentence Transformers.\"\"\"\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "def chunk_text(text, chunk_size=150, overlap=20):\n",
    "    \"\"\"Split text into overlapping chunks of roughly equal size.\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:  # Make sure we're not adding empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def generate_embeddings(video_id):\n",
    "    \"\"\"Generates embeddings from YouTube video transcript.\"\"\"\n",
    "    transcript = extract_transcript(video_id)\n",
    "    if not transcript:\n",
    "        return None, None\n",
    "    \n",
    "    # Preprocess the transcript\n",
    "    processed_transcript = preprocess_text(transcript)\n",
    "    \n",
    "    # Perform POS tagging and analysis\n",
    "    pos_analysis = pos_tagging(processed_transcript)\n",
    "    \n",
    "    # Split into proper chunks for embeddings - use word-based chunking for more control\n",
    "    text_chunks = chunk_text(processed_transcript)\n",
    "    \n",
    "    print(f\"Created {len(text_chunks)} text chunks from transcript\")\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = []\n",
    "    for chunk in text_chunks:\n",
    "        if len(chunk.strip()) > 10:  # Only process non-empty chunks\n",
    "            embedding = get_embedding(chunk)\n",
    "            embeddings.append(embedding)\n",
    "    \n",
    "    print(f\"Generated {len(embeddings)} embeddings\")\n",
    "    \n",
    "    return np.array(embeddings, dtype=np.float32), pos_analysis\n",
    "\n",
    "def store_embeddings_in_pinecone(embeddings, video_id, pos_analysis=None):\n",
    "    \"\"\"Store embeddings in Pinecone index with metadata.\"\"\"\n",
    "    if embeddings is not None and len(embeddings) > 0:\n",
    "        ids = [f\"{video_id}_{i}\" for i in range(len(embeddings))]\n",
    "        \n",
    "        # Format vectors for Pinecone upsert\n",
    "        vectors = []\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            # Create basic metadata for each chunk\n",
    "            metadata = {\n",
    "                \"video_id\": video_id,\n",
    "                \"chunk_id\": i\n",
    "            }\n",
    "            \n",
    "            # Add full POS analysis only to the first vector to avoid duplication\n",
    "            if i == 0 and pos_analysis:\n",
    "                metadata.update({\n",
    "                    \"pos_distribution\": str(pos_analysis[\"pos_distribution\"]),\n",
    "                    \"common_entities\": str(pos_analysis[\"entities\"][:10] if len(pos_analysis[\"entities\"]) > 10 else pos_analysis[\"entities\"]),\n",
    "                    \"common_verbs\": str(pos_analysis[\"common_verbs\"])\n",
    "                })\n",
    "            \n",
    "            vector_entry = {\n",
    "                \"id\": ids[i], \n",
    "                \"values\": emb.tolist(), \n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            vectors.append(vector_entry)\n",
    "            \n",
    "        # Upsert in batches to avoid size limitations\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(vectors), batch_size):\n",
    "            batch = vectors[i:i+batch_size]\n",
    "            index.upsert(vectors=batch)\n",
    "            print(f\"Upserted batch {i//batch_size + 1}/{(len(vectors)-1)//batch_size + 1} to Pinecone\")\n",
    "            \n",
    "        print(f\"Successfully upserted {len(vectors)} embeddings into Pinecone.\")\n",
    "        \n",
    "        # If POS analysis exists, print summary\n",
    "        if pos_analysis:\n",
    "            print(\"\\nPOS Analysis Summary:\")\n",
    "            print(f\"- Most common parts of speech: {dict(Counter(pos_analysis['pos_distribution']).most_common(5))}\")\n",
    "            print(f\"- Top entities detected: {pos_analysis['entities'][:5] if pos_analysis['entities'] else 'None'}\")\n",
    "            print(f\"- Sample noun phrases: {pos_analysis['noun_phrases'][:5] if pos_analysis['noun_phrases'] else 'None'}\")\n",
    "            print(f\"- Most common verbs: {list(pos_analysis['common_verbs'].keys())[:5] if pos_analysis['common_verbs'] else 'None'}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process YouTube videos.\"\"\"\n",
    "    yt_video_id = \"CqOfi41LfDw\"  # Replace with actual YouTube video ID\n",
    "    \n",
    "    print(f\"Processing YouTube video: {yt_video_id}\")\n",
    "    print(\"Extracting transcript and generating embeddings...\")\n",
    "    \n",
    "    embeddings, pos_analysis = generate_embeddings(yt_video_id)\n",
    "    \n",
    "    if embeddings is not None and len(embeddings) > 0:\n",
    "        print(f\"Generated {len(embeddings)} embedding chunks\")\n",
    "        store_embeddings_in_pinecone(embeddings, yt_video_id, pos_analysis)\n",
    "    else:\n",
    "        print(\"Failed to generate embeddings\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
